Pour ce projet, on m'a chargé de trois missions : Etudier la faisabilité d'un moteur de classification automatique à partir du texte et de l'image, réaliser une classification supervisée à partir des images, et tester la collecte de produits à base de “champagne” via l'API Openfood Facts. Il y a donc un notebook par mission. Sur une marketplace e-commerce, les vendeurs proposent des articles en postant une photo et une description. L'attribution de la catégorie d'un article est effectuée manuellement par les vendeurs, et donc peu fiable. Pour faciliter la mise en ligne de nouveaux articles et la recherche de produits, il est nécessaire d'automatiser cette tâche d'attribution de catégorie. C'est ici qu'on se lance dans le premier notebook, il y a deux grosses parties de prétraitement : les données textuelles (les descriptions), puis les données images. Pour les données textuelles, je procède à un nettoyage (retrait de la ponctuation et des stopwords), je tokenize pour tester deux options de racinalisation : lemmatisation et stemming. Je retire les mots qui apparaissent soit trop peu (bruit) trop souvent (peu informatifs) dans le corpus. Suite à cela je teste différentes méthodes pour l'extraction de features : Des bag-of-words (comptage simple et TF-IDF), des word/sentence embeddings (Word2Vec, BERT et USE). Pour les données images, je procède à un retraitement via : conversion en niveaux de gris, réduction du bruit, égalisation d'histogramme et floutage. Suite à cela je teste deux méthodes pour l'extraction de features : SIFT et CNN/Transfer Learning (en m'appuyant sur le modèle VGG-16). Bien évidemment, j'ai conscience que certaines de ces méthodes sont un peu obsolètes, pour la suite de l'étude de faisabilité je me suis focalisé sur BERT, USE et VGG-16. Seulement, avec des jeux de données à respectivement 768, 512 et 4096 colonnes, impossible de visualiser sans réduire la dimensionnalité. Afin d'aboutir à une projection en deux dimensions, j'ai opté pour le t-SNE dont j'ai testé différentes valeurs d'hyperparamètres pour retenir la meilleure combinaison possible par modèle. Pour ce faire, j'ai défini une fonction qui allait faire office de GridSearch et je me suis appuyé sur l'index de Davies-Bouldin et le Silhouette Score commme métriques pour évaluer la qualité des clusters. Enfin, j'ai entraîné un k-means par modèle et calculé l'ARI pour mesurer la similarité entre les clusters réels et les clusters prédits. L'étude de faisabilité étant concluante, on passe au second notebook dans lequel j'ai tenté quatre approches de classification supervisée à partir des images :

    Une approche simple par préparation initiale de l'ensemble des images avant classification supervisée.
    Une approche par data generator, permettant facilement la data augmentation. Les images sont directement récupérées à la volée dans le repertoire des images.
    Une approche récente proposée par Tensorflow.org par DataSet, sans data augmentation.
    Une approche par DataSet, avec data augmentation intégrée au modèle : layer en début de modèle.

Pour chaque modèle, je teste deux critères de surveillances différents pour mesurer leur impact : val_loss (à minimisner) et val_accuracy (à maximiser). Je n'ai pas tout gardé dans le notebook afin de ne pas le noyer d'informations, mais j'ai également testé différentes valeurs de patience (4 et 5), batch_size (64, 32 et 16), optimizer (RMSPROP et ADAM) ainsi que différentes distribution entre jeu d'entraînement, de validation et de test (80-10-10 et 70-15-15). Chaque entraînement de modèle suit le même schéma : Création du modèle, création du callback, entraînement du modèle avec enregistrement du temps d'entraînement et du nombre d'epochs, évaluation des résultats sur le jeu de validation et de test, plot history pour voir l'évolution de la perte et de l'accuracy en fonction de l'epoch, prédictions sur le jeu de test, heatmap et classification report. On finit par une comparaison des résultats entre les modèles. Enfin, le dernier notebook teste la collecte de produits à base de “champagne” via l'API Openfood Facts, les résultats sont enregistrés dans le fichier csv.
